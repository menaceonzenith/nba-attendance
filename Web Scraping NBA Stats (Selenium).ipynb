{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping (Selenium)\n",
    "\n",
    "Below are the codes I used to scrape the data I needed from Basketball-Reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time, os\n",
    "\n",
    "chromedriver = \"/Applications/chromedriver\" # path to the chromedriver executable\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of the 30 NBA teams we will loop for.\n",
    "# Will create 3 separate lists to pull data, since two teams have changed names.\n",
    "\n",
    "# NJN FROM 2010-2012, BRK 2013-2020\n",
    "# NOH FROM 2010-2013, NOP 2014-2020\n",
    "# CHA FROM 2010-2014, CHO 2015-2020\n",
    "# WILL SKIP 2012, AS THE SEASON WAS ONLY 66 GAME (LOCKOUT SEASON)\n",
    "\n",
    "nba_teams = ['BOS', 'TOR', 'NYK', 'PHI', 'CLE', 'MIL', 'CHI', 'IND', 'DET', 'ORL',\n",
    "                 'ATL', 'MIA', 'WAS', 'DEN', 'UTA', 'POR', 'SEA', 'OKC', 'MIN', 'LAL', \n",
    "                 'PHO', 'LAC', 'GSW', 'SAC', 'DAL', 'SAS', 'HOU', 'VAN', 'MEM', 'NJN', 'BRK', 'CHH',\n",
    "                 'NOH','NOK', 'NOP', 'CHA', 'CHO']\n",
    "\n",
    "years = ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2013', '2014', '2015', '2016', '2017', '2018', '2019']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NBA League Stats/Players Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will take all the scraped data and input it into a master list.\n",
    "nba_total_stats = []\n",
    "\n",
    "# Set global driver as Chrome\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "count = 0\n",
    "for each_team in nba_teams:\n",
    "    for each_year in years:\n",
    "        url = f'https://www.basketball-reference.com/teams/{each_team}/{each_year}.html'\n",
    "        \n",
    "        # Use Selenium to access the site and load up the tables needed to extract data.\n",
    "        driver.get(url)\n",
    "        time.sleep(5)\n",
    "        driver.execute_script('window.scrollTo(0, 5000);')\n",
    "        time.sleep(7)\n",
    "        \n",
    "        # Use BeautifulSoup to parse data.\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        team_stats = soup.find('table', id = 'team_and_opponent')\n",
    "        misc_stats = soup.find('table', id = 'team_misc')\n",
    "\n",
    "\n",
    "        if team_stats == None or misc_stats == None or players_stats == None:\n",
    "            continue\n",
    "        else:\n",
    "            rows_team_stats = [row for row in team_stats.find_all('td')]\n",
    "            total_team_stats = rows_team_stats[24:46]\n",
    "            \n",
    "            rows_misc_stats = [row for row in misc_stats.find_all('td')]\n",
    "            total_misc_stats = rows_misc_stats[0:22]\n",
    "            \n",
    "        \n",
    "        # Hold the team stats in a temporary list and append it to master list\n",
    "            temp_list = []\n",
    "            for num_range in range(22):\n",
    "                for each in total_team_stats[num_range]:\n",
    "                    temp_list.append(each)\n",
    "            for num_range in range(22):\n",
    "                for each in total_misc_stats[num_range]:\n",
    "                    temp_list.append(each)\n",
    "        \n",
    "            temp_list = [each_team, each_year] + temp_list\n",
    "            nba_total_stats.append(temp_list)\n",
    "  \n",
    "        # Opened tabs instead of windows.\n",
    "            driver.find_element_by_tag_name('body').send_keys(Keys.COMMAND + 't') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NBA All-Stars Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web-Scrape for NBA All Stars\n",
    "\n",
    "# Master all_stars list.\n",
    "all_stars = []\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "years = ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2013', '2014', '2015', '2016', '2017', '2018', '2019']\n",
    "for each_year in years:\n",
    "    url = f'https://www.basketball-reference.com/allstar/NBA_{each_year}.html'#site_menu_link''\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Use BeautifulSoup to parse data.\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Separate out West and East All-Stars\n",
    "    west = soup.find('table', id = 'West')\n",
    "    east = soup.find('table', id = 'East')\n",
    "    \n",
    "    # Filter for just the individual teams that have all-stars\n",
    "    if west == None or east == None:\n",
    "        lebron = soup.find('table', id = 'LeBron')\n",
    "        stephen = soup.find('table', id = 'Stephen')\n",
    "        if stephen == None:\n",
    "            lebron = soup.find('table', id = 'LeBron')\n",
    "            giannis = soup.find('table', id = 'Giannis')\n",
    "        \n",
    "            lebron_rows = [row for row in lebron.find_all('td')]\n",
    "            lebron_all_stars = lebron_rows[:250:20]\n",
    "        \n",
    "            giannis_rows = [row for row in giannis.find_all('td')]\n",
    "            giannis_all_stars = giannis_rows[:250:20]\n",
    "            \n",
    "            temp_all_stars = []\n",
    "            for each in range(len(lebron_all_stars)):\n",
    "                temp_all_stars.append(lebron_all_stars[each].text)\n",
    "                temp_all_stars.append(giannis_all_stars[each].text)\n",
    "                \n",
    "            temp_all_stars = [each_year] + temp_all_stars\n",
    "            all_stars.append(temp_all_stars)\n",
    "            temp_all_stars = []   \n",
    "            \n",
    "        else:\n",
    "    \n",
    "            lebron_rows = [row for row in lebron.find_all('td')]\n",
    "            lebron_all_stars = lebron_rows[:240:20]\n",
    "        \n",
    "            stephen_rows = [row for row in stephen.find_all('td')]\n",
    "            stephen_all_stars = stephen_rows[:240:20]\n",
    "        \n",
    "            temp_all_stars = []\n",
    "            for each in range(len(lebron_all_stars)):\n",
    "                temp_all_stars.append(lebron_all_stars[each].text)\n",
    "                temp_all_stars.append(stephen_all_stars[each].text)\n",
    "    \n",
    "    \n",
    "            temp_all_stars = [each_year] + temp_all_stars\n",
    "            all_stars.append(temp_all_stars)\n",
    "            temp_all_stars = []\n",
    "            \n",
    "        \n",
    "    else:    \n",
    "        west_rows = [row for row in west.find_all('td')]\n",
    "        west_all_stars = west_rows[:240:20]\n",
    "\n",
    "        east_rows = [row for row in east.find_all('td')]\n",
    "        east_all_stars = east_rows[:240:20]\n",
    "\n",
    "    # Store all stars in a temporary list to append to master all_stars list.\n",
    "        temp_all_stars = []\n",
    "        for each in range(len(west_all_stars)):\n",
    "            temp_all_stars.append(west_all_stars[each].text)\n",
    "            temp_all_stars.append(east_all_stars[each].text)\n",
    "    \n",
    "    \n",
    "        temp_all_stars = [each_year] + temp_all_stars\n",
    "        all_stars.append(temp_all_stars)\n",
    "\n",
    "        temp_all_stars = []\n",
    "        \n",
    "        driver.find_element_by_tag_name('body').send_keys(Keys.COMMAND + 't')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructed a DataFrame to hold all of my scraped team stats.\n",
    "nba_team_stats_df = pd.DataFrame(nba_total_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructed a DataFrame to hold all of my scraped all star players.\n",
    "\n",
    "nba_all_stars = pd.DataFrame(all_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created columns for my DataFrames.\n",
    "nba_team_stats_df.columns = ('Team', 'Year', 'MP','FG', 'FGA','FG%','3P', '3PA' , '3P%',\n",
    "                       '2P', '2PA', '2P%', 'FT', 'FTA', 'FT%', 'ORB', 'DRB', \n",
    "                       'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS', 'WINS',\n",
    "                       'LOSSES', 'PW', 'PL', 'MOV', 'SOS', 'SRS', 'ORTG', 'DRTG',\n",
    "                       'PACE', 'FTR', '3PAR', 'EFG%', 'TOV%', 'ORB%', 'FT/FGA',\n",
    "                       'OEFG%', 'OTOV%', 'ODRB%', 'OFT/FGA', 'ARENA', 'ATTENDANCE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_all_stars.columns = ('Year', 'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9', 'T10', 'T11', 'T12', 'T13',\n",
    "                         'T14', 'T15', 'T16', 'T17', 'T18',' T19', 'T20', 'T21', 'T22', 'T23', 'T24', 'T25', 'T26')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved as a CSV file to have easy access.\n",
    "nba_team_stats_df.to_csv('nba_team_stats.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_all_stars.to_csv('nba_all_stars.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## US Cities Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_years = [2000, 2001, 2002, 2003, 2004,2005,2006,2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018]\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "master_poplist = []\n",
    "\n",
    "for each_year in pop_years:\n",
    "    pop_url = f'https://biggestuscities.com/{each_year}'\n",
    "    driver.get(pop_url)\n",
    "    time.sleep(4)\n",
    "\n",
    "    # Use BeautifulSoup to parse data.\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    table = soup.find('table')\n",
    "    rows2 = [row for row in table.find_all('td')]\n",
    "    \n",
    "    poplist = []\n",
    "    for i in range(3, 5000, 5):\n",
    "        temp_list = []\n",
    "        temp_list.append(each_year)\n",
    "        temp_list.append(rows2[i-2].text.replace('\\n', '').replace('      ', '').replace(' ', ''))\n",
    "        temp_list.append(rows2[i].text.replace('\\n', '').replace('          ', ''))\n",
    "        master_poplist.append(temp_list)\n",
    " \n",
    "        \n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>NewYork</td>\n",
       "      <td>8,015,348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>LosAngeles</td>\n",
       "      <td>3,703,921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>2,895,671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000</td>\n",
       "      <td>Houston</td>\n",
       "      <td>1,977,811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>1,513,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18995</th>\n",
       "      <td>2018</td>\n",
       "      <td>PlantCity</td>\n",
       "      <td>39,156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18996</th>\n",
       "      <td>2018</td>\n",
       "      <td>Norwich</td>\n",
       "      <td>39,136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18997</th>\n",
       "      <td>2018</td>\n",
       "      <td>Germantown</td>\n",
       "      <td>39,099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18998</th>\n",
       "      <td>2018</td>\n",
       "      <td>Northglenn</td>\n",
       "      <td>39,010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18999</th>\n",
       "      <td>2018</td>\n",
       "      <td>Hurst</td>\n",
       "      <td>38,992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0             1                2\n",
       "0      2000       NewYork  8,015,348      \n",
       "1      2000    LosAngeles  3,703,921      \n",
       "2      2000       Chicago  2,895,671      \n",
       "3      2000       Houston  1,977,811      \n",
       "4      2000  Philadelphia  1,513,800      \n",
       "...     ...           ...              ...\n",
       "18995  2018     PlantCity     39,156      \n",
       "18996  2018       Norwich     39,136      \n",
       "18997  2018    Germantown     39,099      \n",
       "18998  2018    Northglenn     39,010      \n",
       "18999  2018         Hurst     38,992      \n",
       "\n",
       "[19000 rows x 3 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(master_poplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(master_poplist).to_csv('city_pop.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
